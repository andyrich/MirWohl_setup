{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freyberg Model PEST setup example\n",
    "Herein, we will show users how to use pyEMU to setup a groundwater model for use in pest.  We will cover the following topics:\n",
    "- setup pilot points as parameters, including 1st-order tikhonov regularization\n",
    "- setup other model inputs as parameters\n",
    "- setup simulated water levels as observations\n",
    "- setup simulated water budget components as observations (or forecasts)\n",
    "- create a pest control file and adjust observation weights to balance the objective function\n",
    "\n",
    "Note that, in addition to `pyemu`, this notebook relies on `flopy`. `flopy` can be obtained (along with installation instructions) at https://github.com/modflowpy/flopy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import shutil\n",
    "import platform\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle as rect\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib as mpl\n",
    "newparams = {'legend.fontsize':10, 'axes.labelsize':10,\n",
    "             'xtick.labelsize':10, 'ytick.labelsize':10,\n",
    "             'font.family':'Univers 57 Condensed', \n",
    "             'pdf.fonttype':42}\n",
    "plt.rcParams.update(newparams)\n",
    "import pyemu\n",
    "from pathlib import Path\n",
    "import basic\n",
    "import flopy\n",
    "import conda_scripts.utils.folium_maps as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pathing trickery to make sure we can run this whole nb in a sandbox (and to make sure we dont break paths if we keep running this cell!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# egpath = Path(\".\").absolute()\n",
    "# while egpath.name != 'examples':\n",
    "#     os.chdir('..')\n",
    "#     egpath = Path(\".\").absolute()\n",
    "\n",
    "# model_ws = Path(\"Freyberg_transient\").absolute()\n",
    "# tmp_path = Path(\"modflow_to_pest_like_a_boss\").absolute()\n",
    "\n",
    "# EXE_DIR = Path(\"..\",\"bin\").absolute()\n",
    "# if \"window\" in platform.platform().lower():\n",
    "#     EXE_DIR = Path(EXE_DIR,\"win\")\n",
    "# elif \"darwin\" in platform.platform().lower() or \"macos\" in platform.platform().lower():\n",
    "#     EXE_DIR = Path(EXE_DIR,\"mac\")\n",
    "# else:\n",
    "#     EXE_DIR = Path(EXE_DIR,\"linux\")\n",
    "    \n",
    "# basename = Path(model_ws).name\n",
    "# new_d = Path(tmp_path, basename)\n",
    "# if new_d.exists():\n",
    "#     shutil.rmtree(new_d)\n",
    "# Path(tmp_path).mkdir(exist_ok=True)\n",
    "# # creation functionality\n",
    "# shutil.copytree(model_ws, new_d)\n",
    "\n",
    "# os.chdir(tmp_path)\n",
    "# print(Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model background\n",
    "This example is based on the synthetic classroom model of Freyberg(1988).  The  model is a 2-dimensional MODFLOW model with 1 layer,  40 rows, and 20 columns.  The model has 2 stress periods: an initial steady-state stress period used for calibration, and a 5-year transient stress period.  The calibration period uses the recharge and well flux of Freyberg(1988); the last stress period use 25% less recharge and 25% more pumping to represent future conditions for a forecast period.\n",
    "\n",
    "Freyberg, David L. \"AN EXERCISE IN GROUND‚ÄêWATER MODEL CALIBRATION AND PREDICTION.\" Groundwater 26.3 (1988): 350-360."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = basic.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load the existing model and save it in a new dir and make sure it runs\n",
    "\n",
    "# model_ws = new_d.relative_to(tmp_path)\n",
    "# ml = flopy.modflow.Modflow.load(\"freyberg.nam\",model_ws=model_ws,verbose=False)\n",
    "# ml.model_ws = \"temp\"\n",
    "# ml.exe_name = \"mfnwt\"\n",
    "# [shutil.copy2(os.path.join(EXE_DIR,f),os.path.join(ml.model_ws,f)) for f in os.listdir(EXE_DIR)]\n",
    "# ml.write_input()\n",
    "# pyemu.os_utils.run(\"mfnwt freyberg.nam\", cwd=ml.model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "## HOB observations\n",
    "Here we are going to setup an ``hob`` package to handle getting the observations from modflow.  Normally, you would already have this file made, but here we are just making one for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hds = flopy.utils.HeadFile(os.path.join(ml.model_ws,\"freyberg.hds\"))\n",
    "# rc_df = pd.read_csv(Path(egpath, \"Freyberg\",\"misc\",\"obs_rowcol.dat\"),delim_whitespace=True)\n",
    "# data = hds.get_alldata()\n",
    "# obs = []\n",
    "# roff = 0.0#ml.dis.delc.array[0] / 2.0\n",
    "# coff = 0.0#ml.dis.delr.array[0] / 2.0\n",
    "# for n,r,c in zip(rc_df.name,rc_df.row,rc_df.col):\n",
    "#     name = \"i{1:02d}j{2:02d}\".format(n,r-1,c-1)\n",
    "#     d = np.zeros((data.shape[0]-1,2))\n",
    "#     d[:,0] = hds.times[1:]\n",
    "#     d[:,1] = data[1:,0,r-1,c-1] + np.random.randn(d.shape[0]) #add some random noise to the observations\n",
    "#     obs.append(flopy.modflow.HeadObservation(ml,obsname=name,layer=0,row=r-1,\n",
    "#                                   column=c-1,roff=roff,coff=coff,\n",
    "#                                   time_series_data=d))\n",
    "# flopy.modflow.ModflowHob(ml,obs_data=obs,iuhobsv=600)\n",
    "# ext_path = os.path.join(ml.model_ws,\"ref\")\n",
    "# if os.path.exists(ext_path):\n",
    "#     shutil.rmtree(ext_path)\n",
    "# print(ext_path)\n",
    "# os.mkdir(ext_path)\n",
    "# ml.external_path = os.path.split(ext_path)[-1]\n",
    "# ml.upw.hk.fmtin = \"(FREE)\"\n",
    "# ml.upw.sy.fmtin = \"(FREE)\"\n",
    "# ml.rch.rech.fmtin = \"(FREE)\"\n",
    "# ml.write_input()\n",
    "# pyemu.os_utils.run(\"mfnwt freyberg.nam\", cwd=ml.model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``pyemu`` has a helper function to setup this instruction file for you and also load observations into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob_df = pyemu.gw_utils.modflow_hob_to_instruction_file(os.path.join(ml.model_ws,ml.name+\".hob.out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe returned has a lot of useful info that we will use later...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list file budget components as observations (or forecasts)\n",
    "\n",
    "Here we will use ``flopy`` and ``pyemu`` to load each of the flux and volume budget components from the ``modflow`` list file to use as observations (or forecasts).  These are valuable pieces of information and since observations are free, why not include them?  This helper function writes two instruction files: ``<flx_filename>.ins`` and ``<vol_filename>.ins``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the flux budget output filename that will be written during each forward run\n",
    "# flx_filename=os.path.join(ml.model_ws,\"flx.out\")\n",
    "\n",
    "# # the volumne budget output filename that will be written during each forward run\n",
    "# vol_filename = os.path.join(ml.model_ws,\"vol.out\")\n",
    "# df_wb = pyemu.gw_utils.setup_mflist_budget_obs(os.path.join(ml.model_ws,ml.name+\".list\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_wb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "## pilot points\n",
    "\n",
    "Here we will setup pilot points for several array-based ``modflow`` inputs using ``pyemu``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup pilot point locations\n",
    "\n",
    "first specify what pilot point names we want to use for each model layer (counting from 0).  Here we will setup pilot points for ``hk``, ``sy`` and ``rech``.  The ``rech`` pilot points will be used as a single multiplier array for all stress periods to account for potential spatial bias in recharge.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dict= {0:[\"hk1\",\"sy1\",\"ss1\"],\n",
    "             1:[\"hk2\",\"sy2\",\"ss2\"],\n",
    "             2:[\"hk3\",\"ss3\"]}\n",
    "ppfolder = 'pilot_points'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function is doing a lot of things: writing templates, pilot point files, and creating a shapefile of pilot points.  The ``every_n_cell`` arg is key: it decides how many cells to skip between pilot point locations - since we passed the ``model``, only active model cells get pilot points (using ``bas6.ibound``).  Like many things with ``flopy``, the ``SpatialReference`` is used to define pilot point ``x`` and ``y `` coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show map of areas where active pilot points are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "ib= copy.deepcopy(ml.bas6.ibound.array[0])\n",
    "\n",
    "i, j = np.indices(ib.shape)\n",
    "\n",
    "ib[i<115] = 0\n",
    "plt.imshow(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr = pyemu.helpers.SpatialReference.from_namfile(os.path.join(ml.model_ws, ml.namefile),\n",
    "#                                                  delr=ml.dis.delr, delc=ml.dis.delc)\n",
    "sr = pyemu.helpers.SpatialReference(\n",
    "                                    delr=ml.dis.delr,\n",
    "                                    delc=ml.dis.delc,\n",
    "                                    xll=6296111.0,\n",
    "                                    yll=1940784.0,\n",
    "                                    rotation=0.0, \n",
    "                                    proj4_str='epsg:2226', \n",
    "                                    units='feet',\n",
    "                                    lenuni=1)\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_cells = 7\n",
    "pp_df = pyemu.pp_utils.setup_pilotpoints_grid(\n",
    "                                              sr = sr,\n",
    "                                                ibound = ml.bas6.ibound.array,\n",
    "                                                # ibound = ib,\n",
    "                                              prefix_dict=prefix_dict,\n",
    "                                              every_n_cell=pp_cells,\n",
    "                                              pp_dir=os.path.join(ml.model_ws, ppfolder),\n",
    "                                           tpl_dir=os.path.join(ml.model_ws, ppfolder),\n",
    "                                              shapename=os.path.join(ml.model_ws,ppfolder,\"pp.shp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``dataframe`` return has the same info as the shapefile that was written - useful info, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp_df.loc[:,'x'] = ml.modelgrid.get_xcellcenters_for_layer(0)[\n",
    "#     pp_df.loc[:,'i'].values,pp_df.loc[:,'j'].values]\n",
    "# pp_df.loc[:,'y'] = ml.modelgrid.get_ycellcenters_for_layer(0)[\n",
    "#     pp_df.loc[:,'i'].values,pp_df.loc[:,'j'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = gpd.read_file(os.path.join(ml.model_ws, ppfolder, 'pp.shp'))\n",
    "print(tab.groupby(['pargp']).count().loc[:,['name']])\n",
    "print(f\"total points {tab.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab.crs = 2226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tab.query('k==0').explore(name = 'layer1')\n",
    "tab.query('k==1').explore(name = 'layer2',m=m)\n",
    "tab.query('k==2').explore(name = 'layer3',m=m)\n",
    "fm.add_layers(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df.index = pp_df.parnme\n",
    "pp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### geostats and kriging\n",
    "now that we have pilot points setup, we need to solve the kriging equations for each model cell using pilot point locations.  Since we only have a single set of pilot points that we are reusing for several array-based ``modflow`` inputs, we only need to get the kriging factors once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hk_pp = pyemu.pp_utils.pp_file_to_dataframe(os.path.join(ml.model_ws,ppfolder,\"hk1pp.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hk_pp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup a geostatistical structure.  The contribution doesn't matter for pilot point interpolation, but it does matter when we want to form a prior parameter covariance matrix - we will get to that later.  A good rule of thumb is to use an ``a`` value that is three times the pilot point spacing.  Also, since the all of these pilot points will be log transformed, we need to use a log-based geostatistical structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pp_cells * ml.dis.delr.array[0] * 3.0\n",
    "v = pyemu.geostats.ExpVario(contribution=1.0,a=a)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=v,transform=\"log\")\n",
    "gs.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where things get fun.  First we create an ``OrdinaryKrige`` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok = pyemu.geostats.OrdinaryKrige(geostruct=gs,point_data=hk_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use a helper function to solve the kriging factors for each active model cell: ``OrdinaryKrige.calc_factors_grid()`` includes all the standard kriging arguments, such as search radius, min and max interpolation points,zone_array, as well as the option to save the kriging variance array \n",
    "\n",
    "Note: we need to pass out model's spatial reference information. For flopy this used to be contained in model.sr\n",
    "      However this has been superseded by model.modelgrid. To avoid reliance on a changing (and not always backward\n",
    "      compatible) code base the sr method has been abstracted into pyemu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.modelgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.bas6.ibound[0].array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.join(ml.model_ws, ppfolder)\n",
    "for lay in [0, 1, 2]:\n",
    "    \n",
    "    hk_pp = pyemu.pp_utils.pp_file_to_dataframe(os.path.join(ml.model_ws,ppfolder,f\"hk{lay+1}pp.dat\"))\n",
    "    ok = pyemu.geostats.OrdinaryKrige(geostruct=gs,point_data=hk_pp)\n",
    "    ok.calc_factors_grid(sr, \n",
    "                         zone_array=ml.bas6.ibound[lay].array,\n",
    "                         var_filename=os.path.join(ml.model_ws,ppfolder,f\"layer{lay+1}_var.dat\"))\n",
    "\n",
    "    ok.to_grid_factors_file(os.path.join(ml.model_ws,ppfolder,f\"pp{lay+1}.fac\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we know that this function is slow for bigly models, but it is super convienent and allows a lot of flexibility.  So, once we have calculated the kriging factors for each active model cell, we need to write this to a factors file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out that kriging variance array...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_arr = np.ma.masked_invalid(np.loadtxt(os.path.join(ml.model_ws,ppfolder,\"layer1_var.dat\")))\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot(111,aspect=\"equal\")\n",
    "ax.pcolormesh(sr.xcentergrid,sr.ycentergrid,var_arr,alpha=0.8)\n",
    "ax.scatter(hk_pp.x, hk_pp.y,marker='o',s=10, fc = 'k')\n",
    "\n",
    "var_arr = np.ma.masked_invalid(np.loadtxt(os.path.join(ml.model_ws,ppfolder,\"layer2_var.dat\")))\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot(111,aspect=\"equal\")\n",
    "ax.pcolormesh(sr.xcentergrid,sr.ycentergrid,var_arr,alpha=0.8)\n",
    "ax.scatter(hk_pp.x, hk_pp.y,marker='o',s=10, fc = 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr.xcentergrid[0,0], sr.ycentergrid[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_pp.iloc[0,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar  = pyemu.utils.geostats.fac2real(os.path.join(ml.model_ws, ppfolder,'hk1pp.dat'), \n",
    "#                               factors_file = os.path.join(ml.model_ws,ppfolder,\n",
    "#                                                           'pp.fac'), out_file = None)\n",
    "\n",
    "# fig, ax = basic.map_river(m =ml, )\n",
    "# ax.pcolormesh(sr.xcentergrid,sr.ycentergrid,np.ma.array(ar,mask =  ar>100),alpha=0.8)\n",
    "# ax.scatter(hk_pp.x, hk_pp.y,marker='o',s=5, c= 'k')\n",
    "# plt.savefig('pilot_points_example.png', dpi = 250)\n",
    "\n",
    "\n",
    "ar  = pyemu.utils.geostats.fac2real(os.path.join(ml.model_ws, ppfolder,'hk3pp.dat'), \n",
    "                              factors_file = os.path.join(ml.model_ws,ppfolder,\n",
    "                                                          'pp3.fac'), out_file = None)\n",
    "\n",
    "fig, ax = basic.map_river(m =ml, )\n",
    "ax.pcolormesh(sr.xcentergrid,sr.ycentergrid,np.ma.array(ar,mask =  ar>100),alpha=0.8)\n",
    "ax.scatter(hk_pp.x, hk_pp.y,marker='o',s=5, c= 'k')\n",
    "plt.savefig('pilot_points_example.png', dpi = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml2 = flopy.modflow.Modflow.load(ml.namefile, model_ws = 'flopy_error', \n",
    "#                                  )\n",
    "\n",
    "# np.all(ml2.upw.hk.array==0)\n",
    "# np.all(ml2.upw.vka.array==0)\n",
    "\n",
    "# pp_file = \"hk1pp.dat\"\n",
    "# lay = ''.join(filter(str.isdigit, pp_file.strip('.dat')))\n",
    "# lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = basic.load_mod_props(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = 'versions\\sfr_width_test'\n",
    "\n",
    "basic.plot_aquifer_props_pilot_points(ml, ppfolder, out_folder, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_calibrated_props(ml, load_best = True):\n",
    "#     if load_best:\n",
    "#         pv = pd.read_csv(os.path.join(ml.model_ws, ppfolder, 'pval.PVAL'), sep = '\\s+').rename(columns = {'11':'pval'})\n",
    "#         pval = pv.rename(lambda x:x.lower()).pval.to_dict()\n",
    "#     else:\n",
    "#         pval = ml.mfpar.pval.pval_dict\n",
    "    \n",
    "#     hk1= np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'hk1.txt' ))\n",
    "#     hk2= np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'hk2.txt' ))\n",
    "#     hk3= np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'hk3.txt' ))\n",
    "    \n",
    "#     hk1 = hk1*pval['hk_1']\n",
    "#     hk2 = hk2*pval['hk_2']\n",
    "#     hk3 = hk3*pval['hk_3']\n",
    "    \n",
    "#     vk1 = hk1*pval['vk_1']\n",
    "#     vk2 = hk2*pval['vk_2']\n",
    "#     vk3 = hk3*pval['vk_3']\n",
    "    \n",
    "#     ss1 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'ss1.txt' ))\n",
    "#     ss2 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'ss2.txt' ))\n",
    "#     ss3 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'ss3.txt' ))\n",
    "#     ss1 = ss1* pval['ss_1']\n",
    "#     ss2 = ss2* pval['ss_2']\n",
    "#     ss3 = ss3* pval['ss_3']\n",
    "    \n",
    "#     sy1 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'sy1.txt' )) *  pval['sy_1']\n",
    "#     sy2 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'sy2.txt' )) *  pval['sy_2']\n",
    "    \n",
    "    \n",
    "#     return {'hk1':hk1,'hk2':hk2, 'hk3':hk3,\n",
    "#             'vk1':vk1, 'vk2':vk2, 'vk3':vk3,\n",
    "#             'ss1':ss1, 'ss2':ss2, 'ss3':ss3,\n",
    "#             'sy1':sy1, 'sy2':sy2,}, pval\n",
    "\n",
    "# ppar, pval  = plot_calibrated_props(ml)\n",
    "\n",
    "# basic.plot_aquifer_prop(ml, np.stack([ppar['hk1'], ppar['hk2'], ppar['hk3'], ]), vmax = .1)\n",
    "# plt.savefig(os.path.join(ml.model_ws,ppfolder, 'pp_hk.png'), dpi = 250)\n",
    "# plt.savefig(os.path.join(out_folder, 'hk.png'), dpi = 250)\n",
    "# basic.plot_aquifer_prop(ml, np.stack([ppar['vk1'], ppar['vk2'], ppar['vk3'], ]), vmin = 1e-7, vmax = .01, title='Vertical Conductivity')\n",
    "# plt.savefig(os.path.join(ml.model_ws,ppfolder, 'pp_vk.png'), dpi = 250)\n",
    "# plt.savefig(os.path.join(out_folder, 'vk.png'), dpi = 250)\n",
    "# basic.plot_aquifer_prop(ml, np.stack([ppar['ss1'], ppar['ss2'], ppar['ss3'], ]), vmin = 1e-6, vmax = 0.01, title='Specific Storage')\n",
    "# plt.savefig(os.path.join(ml.model_ws,ppfolder, 'pp_ss.png'), dpi = 250)\n",
    "# plt.savefig(os.path.join(out_folder, 'ss.png'), dpi = 250)\n",
    "# basic.plot_aquifer_prop(ml, np.stack([ppar['sy1'], ppar['sy2'], ppar['sy2']*0, ]), vmin = .01, vmax = .3, title='Specific Yield')\n",
    "# plt.savefig(os.path.join(ml.model_ws,ppfolder, 'pp_sy.png'), dpi = 250)# basic.plot_aquifer_prop(ml, np.stack([ppar['hk1'], ar['hk1'], ar['hk1'], ]), vmax = .01)\n",
    "# plt.savefig(os.path.join(out_folder, 'sy.png'), dpi = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_mod_props(ml, load_best = True):\n",
    "#     if load_best:\n",
    "#         pv = pd.read_csv(os.path.join(ml.model_ws, ppfolder, 'pval.PVAL'), sep = '\\s+').rename(columns = {'11':'pval'})\n",
    "#         pval = pv.rename(lambda x:x.lower()).pval.to_dict()\n",
    "#     else:\n",
    "#         pval = ml.mfpar.pval.pval_dict\n",
    "    \n",
    "#     hk1= np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'hklay1_thck.txt' ), delimiter = ',')\n",
    "#     hk2= np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'hklay2_thck.txt' ), delimiter = ',')\n",
    "#     hk3= np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'hklay3_thck.txt' ), delimiter = ',')\n",
    "    \n",
    "#     hk1 = hk1*pval['hk_1']\n",
    "#     hk2 = hk2*pval['hk_2']\n",
    "#     hk3 = hk3*pval['hk_3']\n",
    "    \n",
    "#     vk1 = hk1*pval['vk_1']\n",
    "#     vk2 = hk2*pval['vk_2']\n",
    "#     vk3 = hk3*pval['vk_3']\n",
    "    \n",
    "#     ss1 = np.ones((ml.dis.nrow, ml.dis.ncol)) *  pval['ss_1']\n",
    "#     ss2 = np.ones((ml.dis.nrow, ml.dis.ncol)) *  pval['ss_2']\n",
    "#     ss3 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'sslay3_thck.txt' ), delimiter = ',')\n",
    "#     ss3 = ss3* pval['ss_3']\n",
    "    \n",
    "#     sy1 = np.ones((ml.dis.nrow, ml.dis.ncol)) *  pval['sy_1']\n",
    "#     sy2 = np.ones((ml.dis.nrow, ml.dis.ncol)) *  pval['sy_2']\n",
    "    \n",
    "    \n",
    "#     return {'hk1':hk1,'hk2':hk2, 'hk3':hk3,\n",
    "#             'vk1':vk1, 'vk2':vk2, 'vk3':vk3,\n",
    "#             'ss1':ss1, 'ss2':ss2, 'ss3':ss3,\n",
    "#             'sy1':sy1, 'sy2':sy2,}, pval\n",
    "\n",
    "# ar, pval  = load_mod_props(ml)\n",
    "\n",
    "# basic.plot_aquifer_prop(ml, np.stack([ar['hk1'], ar['hk2'], ar['hk3'], ]), vmax = .1)\n",
    "# plt.savefig(os.path.join(ml.model_ws,ppfolder, 'calib_hk.png'), dpi = 250)\n",
    "# basic.plot_aquifer_prop(ml, np.stack([ar['vk1'], ar['vk2'], ar['vk3'], ]), vmin = 1e-5, vmax = .01, title='Vertical Conductivity')\n",
    "# plt.savefig(os.path.join(ml.model_ws,ppfolder, 'calib_vk.png'), dpi = 250)\n",
    "# basic.plot_aquifer_prop(ml, np.stack([ar['ss1'], ar['ss2'], ar['ss3'], ]), vmin = 1e-5, vmax = .01, title='Specific Storage')\n",
    "# plt.savefig(os.path.join(ml.model_ws,ppfolder, 'calib_ss.png'), dpi = 250)\n",
    "# basic.plot_aquifer_prop(ml, np.stack([ar['sy1'], ar['sy2'], ar['sy2']*0, ]), vmin = 1e-5, vmax = .01, title='Specific Yield')\n",
    "# plt.savefig(os.path.join(ml.model_ws,ppfolder, 'calib_sy.png'), dpi = 250)# basic.plot_aquifer_prop(ml, np.stack([ar['hk1'], ar['hk1'], ar['hk1'], ]), vmax = .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the pilot point values from the aquifer property arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, dfi in pp_df.groupby('pargp'):\n",
    "    # print(group)\n",
    "    pp_df.loc[dfi.index, 'parval1'] = ar[group][dfi.loc[:,'i'],dfi.loc[:,'j']]\n",
    "    \n",
    "    if (pp_df.loc[dfi.index, 'parval1']==0).sum()>0:\n",
    "        \n",
    "        \n",
    "        zeros = pp_df.loc[(pp_df.loc[dfi.index, 'parval1']==0.0),:].index\n",
    "        warnings.warn( f\"there are {len(zeros)} zeros in {group}. These will be filled with the group average\")\n",
    "        \n",
    "        print(zeros)\n",
    "        \n",
    "        # pp_df.loc[zeros,'parval1'] = pp_df.loc[dfi.index, 'parval1'].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df.loc[pp_df.parnme.str.startswith('ss3'),'parval1'] = pp_df.loc[pp_df.parnme.str.startswith('ss2'),'parval1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df.loc[pp_df.parnme.str.startswith('ss3'),'parval1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ar['ss3'].reshape((-1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other inputs as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we rarely know any model inputs perfectly, it is advisable to subject them to adjustment...not to get a good ``fit``, but so we can account for there contribution to uncertainty...How about the conductance between the surface water and groundwater systems.  In this model, we are using ``drain`` type boundaries.  So, let's setup a multiplier parameter for each ``drain`` cell's conductance.  \n",
    "\n",
    "Since we told ``flopy`` to write external files, all of the list-type ``modflow`` inputs are also external, which makes this so much easier!  The first thing to do is copy the orginal drain list files (and all other files in the external directory) to a safe place:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_path = os.path.join(ml.model_ws,\"ref\")\n",
    "# ext_files = [f for f in os.listdir(ext_path)]\n",
    "# drain_files = [f for f in ext_files if \"drn\" in f.lower()]\n",
    "# #print(drain_files)\n",
    "# assert len(drain_files) == ml.nper,\"{0},{1}\".format(len(drain_files),ml.nper)\n",
    "# bak_path = os.path.join(ml.model_ws,\"bak\")\n",
    "# if os.path.exists(bak_path):\n",
    "#     shutil.rmtree(bak_path)\n",
    "# os.mkdir(bak_path)\n",
    "# for f in ext_files:\n",
    "#     shutil.copy2(os.path.join(ext_path,f),os.path.join(bak_path,f))\n",
    "# #assert len(os.listdir(bak_path)) == ml.nper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to do is write a template file.  We will also write a generic cooresponding input file that will make testing easier later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drn_df = pd.read_csv(os.path.join(bak_path,drain_files[0]),\n",
    "#                      header=None,names=[\"l\",\"r\",\"c\",\"stage\",\"cond\"],\n",
    "#                     delim_whitespace=True)\n",
    "# f_tpl = open(os.path.join(ml.model_ws,\"drain_mlt.dat.tpl\"),'w')\n",
    "# f_in = open(os.path.join(ml.model_ws,\"drain_mlt.dat\"),'w')\n",
    "# f_tpl.write(\"ptf ~\\n\")\n",
    "# #build parameter names from model cell info\n",
    "# drn_df.loc[:,\"parnme\"] = drn_df.apply(lambda x: \"drn_i{1:02.0f}j{2:02.0f}\".format(x.l-1,x.r-1,x.c-1),axis=1)\n",
    "# for parnme in drn_df.parnme:\n",
    "#     f_tpl.write(\"{0}  ~   {0}   ~\\n\".format(parnme))\n",
    "#     f_in.write(\"{0}     1.0\\n\".format(parnme))\n",
    "# f_tpl.close()\n",
    "# f_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the pest control file...Finally!\n",
    "\n",
    "Here we will use the template and instruction files to construct a control file.  Then we will use some ``pandas`` magic to set the appropriate parameter and observation info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_files = ['pest\\dataset11a_with_geo.txt.tpl', \n",
    "             'pest\\dataset11a_with_geo_2015.txt.tpl',\n",
    "             'pest\\dataset11a_with_geo_2016.txt.tpl',\n",
    "             'pest\\pval.PVAL.tpl']\n",
    "tpl_files = [os.path.join(ml.model_ws,f) for f in tpl_files]\n",
    "input_files = ['inputs\\dataset11a_with_geo.txt',\n",
    "               'inputs\\dataset11a_with_geo_2015.txt',\n",
    "               'inputs\\dataset11a_with_geo_2016.txt',\n",
    "               'pval.PVAL']\n",
    "input_files = [os.path.join(ml.model_ws,f) for f in input_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_tpl_files = [os.path.join(ml.model_ws,ppfolder,f) for f in \n",
    "                os.listdir(os.path.join(ml.model_ws, ppfolder)) if f.endswith(\".tpl\")]\n",
    "\n",
    "pp_input_files = [f.replace(\".tpl\",'') for f in pp_tpl_files]\n",
    "pp_tpl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_files.extend(pp_tpl_files)\n",
    "input_files.extend(pp_input_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See why it is important to use a consistent naming structure for the templates-input file pairs?  Its the same for the instruction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ins_files = [os.path.join(ml.model_ws,f) for f in os.listdir(ml.model_ws) if f.endswith(\".ins\")]\n",
    "# output_files = [f.replace(\".ins\",'') for f in ins_files]\n",
    "# ins_files\n",
    "\n",
    "ins_files = ['Results\\hobs.out.ins']\n",
    "ins_files = [os.path.join(ml.model_ws,f) for f in ins_files]\n",
    "output_files =  ['Results\\hobs.out']\n",
    "output_files = [os.path.join(ml.model_ws,f) for f in output_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use these files to get a ``pyemu.Pst`` instance.  This object has lots of cool functionality..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst.from_io_files(tpl_files,input_files,ins_files,output_files)\n",
    "\n",
    "pst.model_input_data = pst.model_input_data.applymap(lambda x:x.replace(\"RR_2022\\\\\", ''))\n",
    "pst.model_output_data = pst.model_output_data.applymap(lambda x:x.replace(\"RR_2022\\\\\", ''))\n",
    "display(pst.model_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.model_output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the important parts of the ``Pst`` class.  First, all attributes coorespond to the names in list in the pest manual.  For instance, the ``* parameter data`` section of the control file is a ``pandas.DataFrame`` attribute named ``parameter_data``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the columns of the ``DataFrame`` follow the pest naming conventions.  Its the same for ``* observation data``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What ``pyemu`` has set as the ``obsval`` is the simulated equivalent, if it is available - in the ``pst_from_io_files()`` helper, ``pyemu`` tries to call ``inschek``, and, if successful, loads the output files from ``inschek``.  This can be very handy for error checking in the forward-run process. However, we still need to get the actual observed data into ``obsval``...remember that dataframe from hob processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the ``obsval`` column?  Let's just set the index of this dataframe to ``obsnme``, then pandas does the hard work for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_df = pyemu.gw_utils.modflow_hob_to_instruction_file(\n",
    "    os.path.join(ml.model_ws,'Results','hobs.out'))\n",
    "\n",
    "hob_df.index = hob_df.obsnme\n",
    "hob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[hob_df.index,\"obsval\"] = hob_df.obsval\n",
    "pst.observation_data.loc[hob_df.index,:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_temp = pyemu.Pst(os.path.join(ml.model_ws,\"pp_pest.pst\"))\n",
    "pst_temp.parrep(r\"RR_2022\\pp_pest.par\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_temp = pyemu.Pst(os.path.join(ml.model_ws,\"pst_temp.pst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data = pst_temp.observation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOM!  that was easy...trying doing that without pandas....not fun!\n",
    "\n",
    "We still have a few more items to set to specific values. The biggest one is initial values for parameters - they are given default values of 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par.loc[pp_df.index,\"parval1\"] = pp_df.parval1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pyemu.pp_utils.pp_tpl_to_dataframe(os.path.join(ml.model_ws, ppfolder,\"hk1pp.dat.tpl\"))\n",
    "display(_df.head())\n",
    "\n",
    "cov = gs.covariance_matrix(x=_df.x, y=_df.y, names=_df.parnme)\n",
    "\n",
    "# just for a bit of eye-candy; bright yellow indicates higher covariance.\n",
    "c = plt.imshow(cov.x)\n",
    "plt.colorbar(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can assign prior information equations for preferred difference. Note that the preferred difference = 0, which means our preferred difference regularization is really a preferred *homogeneity* condition! If observation data don't say otherwise, parameters which are close together should be similar to each other.\n",
    "\n",
    "The weights on the prior information equations are the Pearson correlation coefficients implied by the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then assign cov pror\n",
    "pyemu.helpers.first_order_pearson_tikhonov(pst, \n",
    "                                            cov=cov,     # the covariance matrix; these can be for some OR all parameters in pst\n",
    "                                            reset=False, # so as to have both prefered value and prefered difference eqs\n",
    "                                            abs_drop_tol=0.01) # drop pi eqs that have small weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the additional number of prior information equations\n",
    "pst.prior_information.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, gdf in pp_df.query('i<=115').groupby('pargp'):\n",
    "#     print(name+'\\n', gdf,'----------\\n'*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pp above wohler to tied\n",
    "par.loc[pp_df.query('i<=115').index, 'partrans'] = 'tied'\n",
    "\n",
    "# # add a new column named \"partied\" and assign the parameter name to which to tie \"special_par2\"\n",
    "par.loc[:,'partied'] = np.nan\n",
    "\n",
    "# go through each group and assign one as the movable parameter and others tied to it\n",
    "for name, gdf in pp_df.query('i<=115').groupby('pargp'):\n",
    "    first = gdf.index[0]\n",
    "    par.loc[gdf.index, 'partied'] = first\n",
    "    # par.loc[gdf.drop(first).index, 'parval1'] = 1.0\n",
    "    par.loc[first,'partrans'] = 'log'\n",
    "    par.loc[first,'partied'] = np.nan\n",
    "    \n",
    "# # display for comparison; see partrans and partied columns\n",
    "# par.loc[par['pargp'] == 'pargp', ['partrans', 'partied']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_temp.parameter_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par.loc[['ss_1', 'ss_2','ss_3','sy_1', 'sy_2', 'hk_1', 'hk_2', 'hk_3'],'partrans'] = 'fixed'\n",
    "\n",
    "# par.loc[['vk_1', 'vk_2','vk_3'],'parval1'] =[ pval['vk_1'], pval['vk_2'], pval['vk_3']]\n",
    "par.loc[['vk_1', 'vk_2','vk_3'],'parval1'] = pst_temp.parameter_data.loc[['vk_1','vk_2','vk_3'], 'parval1']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix swrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swr = ['swrk','swrk2015', 'swrk2016']\n",
    "par.loc[swr, 'parval1'] = pst_temp.parameter_data.loc[swr, 'parval1']\n",
    "par.loc[swr, 'partrans'] = 'fixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par.partrans.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par.loc[pst_temp.parameter_data.index, 'pargp'] = pst_temp.parameter_data.loc[:, 'pargp'].apply(lambda x: 'pval_'+x).fillna('pargp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = par.sort_values('pargp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par.partied.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, ``pandas`` makes this very easy.  For example, let's set the ``DRN`` conductance parameters to have initial values of mean of the values in the model currently:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(par.loc[:,'parval1']==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab.loc[tab.loc[:,'parnme'].isin(par.loc[(par.loc[:,'parval1']==0)].index)].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg = ml.drn.stress_period_data[0][\"cond\"].mean()\n",
    "# par = pst.parameter_data #just a pointer to the full, long-named attribute\n",
    "# drn_pars = par.loc[par.parnme.apply(lambda x: x.startswith(\"drn\")),\"parnme\"].values\n",
    "# par.loc[drn_pars,\"parval1\"] = avg\n",
    "# #set the par group to mean something\n",
    "# par.loc[drn_pars,\"pargp\"] = \"drn_cond\"\n",
    "# par.loc[drn_pars,\"parubnd\"] = avg * 10.0\n",
    "# par.loc[drn_pars,\"parlbnd\"] = avg * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the ``pargp`` for the remaining parameters using that cool pilot point dataframe from eariler..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par.loc[pp_df.index,\"pargp\"] = pp_df.loc[:,'pargp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reset the model run command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_temp.model_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.model_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that is just a generic command. I prefer to use python scripts for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pst.model_command = [\"python pest\\forward_run_pp.py\"]\n",
    "pst.model_command = pst_temp.model_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this version of the control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this means we need to write ``forward_run.py`` and it needs to perform several actions:\n",
    "- apply kriging factors (using ``pyemu.gw_utils.fac2real()``)\n",
    "- apply the drain multipliers\n",
    "- call ``MODFLOW``\n",
    "- process the ``MODFLOW`` list file\n",
    "\n",
    "Lucky for you, I already made this file...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.copy2(os.path.join(\"Freyberg_transient\",\"forward_run.py\"),os.path.join(ml.model_ws,\"forward_run.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding prior information\n",
    "\n",
    "``pyemu`` supports both zero-order (preferred value) and first-order (preferred difference) Tikhonov regularization.  Let's set preferred value for the conductance parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyemu.utils.helpers.zero_order_tikhonov(pst,par_groups=[\"drn_cond\"])\n",
    "# pst.prior_information.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set preferred difference equations for pilot point groups.  We will use the Pearson coef as the weight..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp_groups = pp_df.groupby(\"pargp\").groups\n",
    "# for pargp,par_names in pp_groups.items():\n",
    "#     this_pp_df = pp_df.loc[par_names,:]\n",
    "#     cov = gs.covariance_matrix(this_pp_df.x,this_pp_df.y,this_pp_df.parnme)\n",
    "#     pyemu.helpers.first_order_pearson_tikhonov(pst,cov,reset=False,abs_drop_tol=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pst.prior_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pst.control_data.pestmode = \"regularization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting PEST++ options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things I like to add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"svd_pack\"] = \"redsvd\"\n",
    "#pst.pestpp_options[\"forecasts\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving the new control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data = par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(ml.model_ws,'ies.pst'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par.loc[:,['pargp','partrans']].groupby('pargp').value_counts(subset = 'partrans').unstack().fillna(0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "ax = sns.stripplot(data=par.query(\"parval1!=1. \").loc[:,['pargp','parval1']], x=\"pargp\", y=\"parval1\")\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True)\n",
    "\n",
    "labels = ax.get_xticklabels()\n",
    "ax.set_xticklabels(labels=labels,rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalpest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mod_props(ml, load_best = True):\n",
    "    if load_best:\n",
    "        pv = pd.read_csv(os.path.join(ml.model_ws, ppfolder, 'pval.PVAL'), sep = '\\s+').rename(columns = {'11':'pval'})\n",
    "        pval = pv.rename(lambda x:x.lower()).pval.to_dict()\n",
    "    else:\n",
    "        pv = pd.read_csv(os.path.join(ml.model_ws, 'pval.PVAL'), sep = '\\s+').rename(columns = {'11':'pval'})\n",
    "        pval = pv.rename(lambda x:x.lower()).pval.to_dict()\n",
    "        # pval = ml.mfpar.pval.pval_dict\n",
    "    \n",
    "    hk1= np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'hk1.txt' ), )\n",
    "    hk2= np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'hk2.txt' ), )\n",
    "    hk3= np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'hk3.txt' ), )\n",
    "    \n",
    "    hk1 = hk1*pval['hk_1']\n",
    "    hk2 = hk2*pval['hk_2']\n",
    "    hk3 = hk3*pval['hk_3']\n",
    "    \n",
    "    vk1 = hk1*pval['vk_1']\n",
    "    vk2 = hk2*pval['vk_2']\n",
    "    vk3 = hk3*pval['vk_3']\n",
    "    \n",
    "    ss1 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'ss1.txt' ),)\n",
    "    ss2 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'ss2.txt' ),)\n",
    "    ss3 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'ss3.txt' ),)\n",
    "    ss3 = ss3* pval['ss_3']\n",
    "    \n",
    "    sy1 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'sy1.txt' ),)\n",
    "    sy2 = np.genfromtxt(os.path.join(ml.model_ws, ppfolder, 'sy2.txt' ),)\n",
    "    \n",
    "    \n",
    "    return {'hk1':hk1,'hk2':hk2, 'hk3':hk3,\n",
    "            'vk1':vk1, 'vk2':vk2, 'vk3':vk3,\n",
    "            'ss1':ss1, 'ss2':ss2, 'ss3':ss3,\n",
    "            'sy1':sy1, 'sy2':sy2,}, pval\n",
    "\n",
    "arpest, pvalpest  = load_mod_props(ml, False)\n",
    "\n",
    "basic.plot_aquifer_prop(ml, np.stack([arpest['hk1'], arpest['hk2'], arpest['hk3'], ]), vmax = .1)\n",
    "plt.savefig(os.path.join(ml.model_ws,ppfolder, 'pphk.png'), dpi = 250)\n",
    "basic.plot_aquifer_prop(ml, np.stack([arpest['vk1'], arpest['vk2'], arpest['vk3'], ]), vmin = 1e-5, vmax = .01, title='Vertical Conductivity')\n",
    "plt.savefig(os.path.join(ml.model_ws,ppfolder, 'ppvk.png'), dpi = 250)\n",
    "basic.plot_aquifer_prop(ml, np.stack([arpest['ss1'], arpest['ss2'], arpest['ss3'], ]), vmin = 1e-5, vmax = .01, title='Specific Storage')\n",
    "plt.savefig(os.path.join(ml.model_ws,ppfolder, 'ppss.png'), dpi = 250)\n",
    "basic.plot_aquifer_prop(ml, np.stack([arpest['sy1'], arpest['sy2'], arpest['sy2']*0, ]), vmin = 1e-5, vmax = .01, title='Specific Yield')\n",
    "plt.savefig(os.path.join(ml.model_ws,ppfolder, 'ppsy.png'), dpi = 250)\n",
    "# basic.plot_aquifer_prop(ml, np.stack([ar['hk1'], ar['hk1'], ar['hk1'], ]), vmax = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write_input_files('RR_2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
